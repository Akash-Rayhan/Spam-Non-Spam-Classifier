{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMN0pWmVf8+WjVtrSNQf6QU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akash-Rayhan/Spam-Non-Spam-Classifier/blob/main/Spam_Non_Spam_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTL4KOVH6Qob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f774699-d4fb-4e52-ddc1-6b3fca9b4dfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "%pip install -q -U urlextract\n",
        "%pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
        "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
        "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
        "SPAM_PATH = os.path.join(\"datasets\", \"spam\")\n",
        "\n",
        "def fetch_spam_data(ham_url=HAM_URL, spam_url=SPAM_URL, spam_path=SPAM_PATH):\n",
        "    if not os.path.isdir(spam_path):\n",
        "        os.makedirs(spam_path)\n",
        "    for filename, url in ((\"ham.tar.bz2\", ham_url), (\"spam.tar.bz2\", spam_url)):\n",
        "        path = os.path.join(spam_path, filename)\n",
        "        if not os.path.isfile(path):\n",
        "            urllib.request.urlretrieve(url, path)\n",
        "        tar_bz2_file = tarfile.open(path)\n",
        "        tar_bz2_file.extractall(path=spam_path)\n",
        "        tar_bz2_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fetch_spam_data()"
      ],
      "metadata": {
        "id": "FZ9nYT266mh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\n",
        "SPAM_DIR = os.path.join(SPAM_PATH, \"spam\")"
      ],
      "metadata": {
        "id": "ah2zTaR_7eoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 10]\n",
        "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 10]"
      ],
      "metadata": {
        "id": "cBtXPvg8-bWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ham_filenames)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R86Pz2rN-t9h",
        "outputId": "bf273bf4-1737-4118-fd80-ea9ecec88557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2500"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(spam_filenames)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XOnkCn3-0xj",
        "outputId": "d175e96a-71f8-4964-fbd2-47805b8a6a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import email\n",
        "import email.policy\n",
        "\n",
        "def load_email(is_spam, filename, spam_path=SPAM_PATH):\n",
        "    directory = \"spam\" if is_spam else \"easy_ham\"\n",
        "    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n",
        "        return email.parser.BytesParser(policy=email.policy.default).parse(f)"
      ],
      "metadata": {
        "id": "N1LBjaMp-9dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\n",
        "spam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]"
      ],
      "metadata": {
        "id": "xjtfJ_r1EuB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_email_structure(email):\n",
        "    if isinstance(email, str):\n",
        "        return email\n",
        "    payload = email.get_payload()\n",
        "    if isinstance(payload, list):\n",
        "        return \"multipart({})\".format(\", \".join([\n",
        "            get_email_structure(sub_email)\n",
        "            for sub_email in payload\n",
        "        ]))\n",
        "    else:\n",
        "        return email.get_content_type()"
      ],
      "metadata": {
        "id": "gWUnjjAhGFOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def structures_counter(emails):\n",
        "    structures = Counter()\n",
        "    for email in emails:\n",
        "        structure = get_email_structure(email)\n",
        "        structures[structure] += 1\n",
        "    return structures"
      ],
      "metadata": {
        "id": "Xe_gIP4NGLOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some emails are actually multipart, with images and attachments"
      ],
      "metadata": {
        "id": "ETRzGt8SHEco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "structures_counter(spam_emails)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b65EmTJjGxuJ",
        "outputId": "8e997a01-83c3-4464-d624-5eb1a235ddfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'text/html': 183,\n",
              "         'text/plain': 218,\n",
              "         'multipart(text/plain, application/octet-stream)': 1,\n",
              "         'multipart(text/html)': 20,\n",
              "         'multipart(text/plain, text/html)': 45,\n",
              "         'multipart(text/plain)': 19,\n",
              "         'multipart(text/html, text/plain)': 1,\n",
              "         'multipart(text/html, application/octet-stream)': 2,\n",
              "         'multipart(multipart(text/html))': 5,\n",
              "         'multipart(text/plain, image/jpeg)': 3,\n",
              "         'multipart(multipart(text/html), application/octet-stream, image/jpeg)': 1,\n",
              "         'multipart(multipart(text/plain, text/html), image/gif)': 1,\n",
              "         'multipart/alternative': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some emails are  'text/html' \n",
        "\n",
        "Some emails are  'text/plain'\n",
        "\n",
        "There are also some mails which have multipart(meaning image, links, html combined)"
      ],
      "metadata": {
        "id": "5Bc1AB0tNNt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = np.array(ham_emails + spam_emails, dtype=object)\n",
        "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Q1tLWYa8KPO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "def html_to_plain_text(html):\n",
        "  return BeautifulSoup(html).get_text()\n"
      ],
      "metadata": {
        "id": "6Fe21etnJWXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BSUfHlZkN5tO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def email_to_text(email):\n",
        "    html = None\n",
        "    for part in email.walk():# walk through multipart of the email\n",
        "        ctype = part.get_content_type()\n",
        "        if not ctype in (\"text/plain\", \"text/html\"):\n",
        "            continue\n",
        "        try:\n",
        "            content = part.get_content()\n",
        "        except: \n",
        "            content = str(part.get_payload()) #get_payload() in case of encoding issue\n",
        "        if ctype == \"text/plain\":\n",
        "            return content\n",
        "        else:\n",
        "            html = content\n",
        "    if html:\n",
        "        return html_to_plain_text(html)"
      ],
      "metadata": {
        "id": "EoRgnFXoQyA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing the text data.**\n",
        "\n",
        "Text cleaning is the first step where we remove those words from the document which may not contribute to the information we want to extract.\n",
        "\n",
        "1.Strip the headers of email messages\n",
        "\n",
        "2.Use the lower case for all words\n",
        "\n",
        "3.Remove punctuation marks\n",
        "\n",
        "4.Replace links with word 'URL'\n",
        "\n",
        "5.Replace digits with word 'NUMBER'\n",
        "\n",
        "6.Removal of stop words\n",
        "\n",
        "7.Remove absurd single characters which are irrelevant in dictionary\n",
        "\n",
        "7.Lemmatization - The context of the sentence is also preserved in lemmatization as opposed to stemming. It takes into consideration the morphological analysis of the words.\n",
        "\n",
        "stem : studies - studi\n",
        "\n",
        "lemmatization : studies - study"
      ],
      "metadata": {
        "id": "VgImGziND0zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import urlextract\n",
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "url_extractor = urlextract.URLExtract()\n",
        "\n",
        "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n",
        "                 replace_urls=True, replace_numbers=True, remove_stop_words=True, \n",
        "                 remove_single_char = True, lemmatization=True):\n",
        "        self.strip_headers = strip_headers\n",
        "        self.lower_case = lower_case\n",
        "        self.remove_punctuation = remove_punctuation\n",
        "        self.replace_urls = replace_urls\n",
        "        self.replace_numbers = replace_numbers\n",
        "        self.remove_stop_words = remove_stop_words\n",
        "        self.remove_single_char = remove_single_char\n",
        "        self.lemmatization = lemmatization\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        X_transformed = []\n",
        "        for email in X:\n",
        "            text = email_to_text(email) or \"\"\n",
        "            if self.lower_case:\n",
        "                text = text.lower()\n",
        "            \n",
        "            if self.replace_urls and url_extractor is not None:# replace links with word'URL'\n",
        "                urls = list(set(url_extractor.find_urls(text)))\n",
        "                urls.sort(key=lambda url: len(url), reverse=True)\n",
        "                for url in urls:\n",
        "                    text = text.replace(url, \" URL \")\n",
        "            \n",
        "            if self.replace_numbers:# replace digits with word'NUMBER'\n",
        "                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
        "            \n",
        "            if self.remove_punctuation:# remove punctuation\n",
        "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
        "            \n",
        "            if self.remove_stop_words:# remove stop words\n",
        "              sentence = text.split()\n",
        "              stop_words = stopwords.words('english')\n",
        "              filtered_sentence = []\n",
        "              for w in sentence:\n",
        "                if w not in stop_words:\n",
        "                  filtered_sentence.append(w)\n",
        "\n",
        "            if self.remove_single_char: # remove single_char\n",
        "              for index, item in enumerate(filtered_sentence):\n",
        "                if item.isalpha() == False: \n",
        "                    del filtered_sentence[index]\n",
        "                elif len(item) == 1:\n",
        "                    del filtered_sentence[index]\n",
        "\n",
        "            word_counts = Counter(filtered_sentence)\n",
        "\n",
        "            if self.lemmatization and lemmatizer is not None: # lemmatization\n",
        "                lemmatize_word_counts = Counter()\n",
        "                for word, count in word_counts.items():\n",
        "                    lemmatize_word = lemmatizer.lemmatize(word)\n",
        "                    lemmatize_word_counts[lemmatize_word] += count\n",
        "                word_counts = lemmatize_word_counts\n",
        "            \n",
        "            X_transformed.append(word_counts)\n",
        "        \n",
        "        return X_transformed"
      ],
      "metadata": {
        "id": "bPCsDBLBbkPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_few = X_train[:2]\n",
        "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
        "X_few_wordcounts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2sw-aIpbsKj",
        "outputId": "fd2440a6-8ff5-48e4-8c80-aa6c684837f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Counter({'chuck': 1, 'murcko': 1, 'wrote': 1, 'stuff': 1, 'yawn': 1}),\n",
              " Counter({'interesting': 1,\n",
              "          'quote': 1,\n",
              "          'URL': 1,\n",
              "          'thomas': 1,\n",
              "          'jefferson': 2,\n",
              "          'examined': 1,\n",
              "          'known': 1,\n",
              "          'superstition': 2,\n",
              "          'word': 1,\n",
              "          'find': 1,\n",
              "          'particular': 1,\n",
              "          'christianity': 3,\n",
              "          'one': 2,\n",
              "          'redeeming': 1,\n",
              "          'feature': 1,\n",
              "          'alike': 1,\n",
              "          'founded': 1,\n",
              "          'fable': 1,\n",
              "          'mythology': 1,\n",
              "          'million': 1,\n",
              "          'innocent': 1,\n",
              "          'men': 1,\n",
              "          'woman': 1,\n",
              "          'child': 1,\n",
              "          'since': 1,\n",
              "          'introduction': 1,\n",
              "          'burnt': 1,\n",
              "          'tortured': 1,\n",
              "          'fined': 1,\n",
              "          'imprisoned': 1,\n",
              "          'effect': 1,\n",
              "          'coercion': 1,\n",
              "          'make': 1,\n",
              "          'half': 2,\n",
              "          'world': 1,\n",
              "          'fool': 1,\n",
              "          'hypocrite': 1,\n",
              "          'support': 1,\n",
              "          'roguery': 2,\n",
              "          'error': 1,\n",
              "          'earth': 1,\n",
              "          'six': 1,\n",
              "          'historic': 1,\n",
              "          'american': 1,\n",
              "          'john': 1,\n",
              "          'remsburg': 1,\n",
              "          'letter': 1,\n",
              "          'william': 1,\n",
              "          'short': 1,\n",
              "          'become': 1,\n",
              "          'perverted': 1,\n",
              "          'system': 1,\n",
              "          'ever': 1,\n",
              "          'shone': 1,\n",
              "          'man': 1,\n",
              "          'absurdity': 1,\n",
              "          'untruth': 1,\n",
              "          'perpetrated': 1,\n",
              "          'upon': 1,\n",
              "          'teaching': 2,\n",
              "          'jesus': 2,\n",
              "          'large': 1,\n",
              "          'band': 1,\n",
              "          'dupe': 1,\n",
              "          'importer': 1,\n",
              "          'led': 1,\n",
              "          'paul': 1,\n",
              "          'first': 1,\n",
              "          'great': 1,\n",
              "          'corrupter': 1})]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have the word counts, and we need to convert them to vectors. For this, we will build another transformer whose **fit()** method will build the vocabulary (an ordered list of the most common words) and whose **transform()** method will use the vocabulary to convert word counts to vectors. The output is a sparse matrix."
      ],
      "metadata": {
        "id": "0bu-QsfaLWTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vocabulary_size=1000):\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "    def fit(self, X, y=None):\n",
        "        total_count = Counter()\n",
        "        for word_count in X:\n",
        "            for word, count in word_count.items():\n",
        "                total_count[word] += min(count, 10)\n",
        "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
        "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        rows = []\n",
        "        cols = []\n",
        "        data = []\n",
        "        for row, word_count in enumerate(X):\n",
        "            for word, count in word_count.items():\n",
        "                rows.append(row)\n",
        "                cols.append(self.vocabulary_.get(word, 0))\n",
        "                data.append(count)\n",
        "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
      ],
      "metadata": {
        "id": "h-Mi9-ulS4C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
        "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)"
      ],
      "metadata": {
        "id": "fDjnHjvjWicx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 62 in the second row, first column, means that the second email contains 62 words that are not part of the vocabulary. The 3 next to it means that the first word in the vocabulary is present 3 times in this email. The 2 next to it means that the second word is present 2 times, and so on."
      ],
      "metadata": {
        "id": "ADEa6qhyIfjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_few_vectors.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ynmw30_PXEbv",
        "outputId": "1342e1d4-8627-4c7e-e5ff-3bacc17125ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1],\n",
              "       [62,  3,  2,  2,  2,  2,  2,  2,  2,  0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can look at the vocabulary to know which words we are talking about"
      ],
      "metadata": {
        "id": "ohxwOOpsJNJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_transformer.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20Fe7smFXNx5",
        "outputId": "9806e38a-09cb-4575-e811-b00b2e9c3901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'christianity': 1,\n",
              " 'jefferson': 2,\n",
              " 'superstition': 3,\n",
              " 'one': 4,\n",
              " 'half': 5,\n",
              " 'roguery': 6,\n",
              " 'teaching': 7,\n",
              " 'jesus': 8,\n",
              " 'chuck': 9,\n",
              " 'murcko': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "preprocess_pipeline = Pipeline([\n",
        "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
        "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
        "])\n",
        "\n",
        "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "P-WRK8woCIce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
        "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=10, verbose=10)\n",
        "score.mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0mswX47CfRi",
        "outputId": "00792cd9-75a4-428c-ecae-7afb2b2f152d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] START .....................................................................\n",
            "[CV] END ................................ score: (test=0.992) total time=   0.3s\n",
            "[CV] START .....................................................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................................ score: (test=0.975) total time=   0.3s\n",
            "[CV] START .....................................................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.6s remaining:    0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................................ score: (test=0.983) total time=   0.3s\n",
            "[CV] START .....................................................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.9s remaining:    0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................................ score: (test=0.983) total time=   0.3s\n",
            "[CV] START .....................................................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    1.3s remaining:    0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................................ score: (test=0.992) total time=   0.3s\n",
            "[CV] START .....................................................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.6s remaining:    0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................................ score: (test=0.971) total time=   0.3s\n",
            "[CV] START .....................................................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    1.9s remaining:    0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................................ score: (test=0.971) total time=   0.3s\n",
            "[CV] START .....................................................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    2.2s remaining:    0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................................ score: (test=1.000) total time=   0.3s\n",
            "[CV] START .....................................................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    2.6s remaining:    0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................................ score: (test=0.992) total time=   0.3s\n",
            "[CV] START .....................................................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    2.9s remaining:    0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................................ score: (test=0.996) total time=   0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    3.3s finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9854166666666668"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix \n",
        "\n",
        "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
        "\n",
        "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
        "log_clf.fit(X_train_transformed, y_train)\n",
        "\n",
        "y_pred = log_clf.predict(X_test_transformed)\n",
        "\n",
        "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
        "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))\n",
        "print(confusion_matrix(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJcWeLQ8Fh2Z",
        "outputId": "4c36d6d6-e4bb-442d-c4ad-592334c23b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 97.80%\n",
            "Recall: 93.68%\n",
            "[[503   2]\n",
            " [  6  89]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The value of false negative(FN) = 6\n",
        "\n",
        "which means actual class = 1(spam) and predict = 0(non-spam)\n",
        "\n",
        "it miscalculated few spam emails as ham emails which will not affect a lot the user\n",
        "\n",
        "the value of false positive(FP) = 2\n",
        "\n",
        "which means actual class = 0(non-spam) and predict = 1(spam)\n",
        "\n",
        "It made ignorable mistake to recognise non-spam messages as spam. It is good the user will barely miss his/her important mails"
      ],
      "metadata": {
        "id": "O6Jer-v1JoWU"
      }
    }
  ]
}